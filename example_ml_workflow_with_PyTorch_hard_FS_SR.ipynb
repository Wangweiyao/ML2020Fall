{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "Here we prepare data as 1 min level kline for BTC from 2019.1.1 to 2020.5.2 in bitfinex exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m pip install -e .. -U\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TZ = 'Asia/Shanghai'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/data'\n",
    "data_platform_list = ['BITFINEX']\n",
    "data_symbol_list = ['BTC']\n",
    "\n",
    "data_df_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for platform in data_platform_list:\n",
    "    for symbol in data_symbol_list:\n",
    "        pkl_file_path = data_path+'/'+symbol+'_USD_'+platform+'_latest.pkl'\n",
    "        pandas_df = pd.read_pickle(pkl_file_path)\n",
    "        #data_df_list.append(pkl_file.add_prefix(platform+'_'+symbol+':'))\n",
    "        data_df_list.append(pandas_df)\n",
    "data = pd.concat(data_df_list, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Enginnering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tactical indicators etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['timestamp'] = data.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 16:00:00+08:00</th>\n",
       "      <td>3850.000000</td>\n",
       "      <td>3850.00000</td>\n",
       "      <td>3849.600000</td>\n",
       "      <td>3849.60000</td>\n",
       "      <td>0.443293</td>\n",
       "      <td>2019-01-01 16:00:00+08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 16:01:00+08:00</th>\n",
       "      <td>3849.500000</td>\n",
       "      <td>3853.00000</td>\n",
       "      <td>3849.400000</td>\n",
       "      <td>3853.00000</td>\n",
       "      <td>9.085920</td>\n",
       "      <td>2019-01-01 16:01:00+08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 16:02:00+08:00</th>\n",
       "      <td>3853.000000</td>\n",
       "      <td>3857.00000</td>\n",
       "      <td>3852.960000</td>\n",
       "      <td>3853.20000</td>\n",
       "      <td>8.213360</td>\n",
       "      <td>2019-01-01 16:02:00+08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 16:03:00+08:00</th>\n",
       "      <td>3853.000000</td>\n",
       "      <td>3853.10000</td>\n",
       "      <td>3851.200000</td>\n",
       "      <td>3852.20000</td>\n",
       "      <td>6.385190</td>\n",
       "      <td>2019-01-01 16:03:00+08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 16:04:00+08:00</th>\n",
       "      <td>3852.200000</td>\n",
       "      <td>3852.30000</td>\n",
       "      <td>3852.200000</td>\n",
       "      <td>3852.30000</td>\n",
       "      <td>0.504622</td>\n",
       "      <td>2019-01-01 16:04:00+08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-07 15:55:00+08:00</th>\n",
       "      <td>19398.841056</td>\n",
       "      <td>19402.00000</td>\n",
       "      <td>19398.841056</td>\n",
       "      <td>19402.00000</td>\n",
       "      <td>8.340088</td>\n",
       "      <td>2020-12-07 15:55:00+08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-07 15:56:00+08:00</th>\n",
       "      <td>19402.690321</td>\n",
       "      <td>19418.62931</td>\n",
       "      <td>19402.690321</td>\n",
       "      <td>19418.62931</td>\n",
       "      <td>2.047179</td>\n",
       "      <td>2020-12-07 15:56:00+08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-07 15:57:00+08:00</th>\n",
       "      <td>19412.000000</td>\n",
       "      <td>19419.00000</td>\n",
       "      <td>19412.000000</td>\n",
       "      <td>19419.00000</td>\n",
       "      <td>0.153671</td>\n",
       "      <td>2020-12-07 15:57:00+08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-07 15:58:00+08:00</th>\n",
       "      <td>19410.000000</td>\n",
       "      <td>19414.00000</td>\n",
       "      <td>19410.000000</td>\n",
       "      <td>19414.00000</td>\n",
       "      <td>0.633814</td>\n",
       "      <td>2020-12-07 15:58:00+08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-07 15:59:00+08:00</th>\n",
       "      <td>19413.779503</td>\n",
       "      <td>19414.00000</td>\n",
       "      <td>19404.000000</td>\n",
       "      <td>19405.00000</td>\n",
       "      <td>2.242484</td>\n",
       "      <td>2020-12-07 15:59:00+08:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>988483 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   open         high           low  \\\n",
       "2019-01-01 16:00:00+08:00   3850.000000   3850.00000   3849.600000   \n",
       "2019-01-01 16:01:00+08:00   3849.500000   3853.00000   3849.400000   \n",
       "2019-01-01 16:02:00+08:00   3853.000000   3857.00000   3852.960000   \n",
       "2019-01-01 16:03:00+08:00   3853.000000   3853.10000   3851.200000   \n",
       "2019-01-01 16:04:00+08:00   3852.200000   3852.30000   3852.200000   \n",
       "...                                 ...          ...           ...   \n",
       "2020-12-07 15:55:00+08:00  19398.841056  19402.00000  19398.841056   \n",
       "2020-12-07 15:56:00+08:00  19402.690321  19418.62931  19402.690321   \n",
       "2020-12-07 15:57:00+08:00  19412.000000  19419.00000  19412.000000   \n",
       "2020-12-07 15:58:00+08:00  19410.000000  19414.00000  19410.000000   \n",
       "2020-12-07 15:59:00+08:00  19413.779503  19414.00000  19404.000000   \n",
       "\n",
       "                                 close    volume                 timestamp  \n",
       "2019-01-01 16:00:00+08:00   3849.60000  0.443293 2019-01-01 16:00:00+08:00  \n",
       "2019-01-01 16:01:00+08:00   3853.00000  9.085920 2019-01-01 16:01:00+08:00  \n",
       "2019-01-01 16:02:00+08:00   3853.20000  8.213360 2019-01-01 16:02:00+08:00  \n",
       "2019-01-01 16:03:00+08:00   3852.20000  6.385190 2019-01-01 16:03:00+08:00  \n",
       "2019-01-01 16:04:00+08:00   3852.30000  0.504622 2019-01-01 16:04:00+08:00  \n",
       "...                                ...       ...                       ...  \n",
       "2020-12-07 15:55:00+08:00  19402.00000  8.340088 2020-12-07 15:55:00+08:00  \n",
       "2020-12-07 15:56:00+08:00  19418.62931  2.047179 2020-12-07 15:56:00+08:00  \n",
       "2020-12-07 15:57:00+08:00  19419.00000  0.153671 2020-12-07 15:57:00+08:00  \n",
       "2020-12-07 15:58:00+08:00  19414.00000  0.633814 2020-12-07 15:58:00+08:00  \n",
       "2020-12-07 15:59:00+08:00  19405.00000  2.242484 2020-12-07 15:59:00+08:00  \n",
       "\n",
       "[988483 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "import talib\n",
    "\n",
    "# Moving averages\n",
    "data['ma5m'] = talib.MA(data['close'], timeperiod = 5) / data['close'] \n",
    "data['ma10m'] = talib.MA(data['close'], timeperiod = 10) / data['close'] \n",
    "data['ma1h'] = talib.MA(data['close'], timeperiod = 60) / data['close'] \n",
    "data['ma4h'] = talib.MA(data['close'], timeperiod = 240) / data['close'] \n",
    "data['ma12h'] = talib.MA(data['close'], timeperiod = 720) / data['close'] \n",
    "data['ma1d'] = talib.MA(data['close'], timeperiod = 1440) / data['close']\n",
    "data['ma5d'] = talib.MA(data['close'], timeperiod = 7200) / data['close'] \n",
    "data['ma10d'] = talib.MA(data['close'], timeperiod = 14400) / data['close'] \n",
    "data['ma30d'] = talib.MA(data['close'], timeperiod = 43200) / data['close'] \n",
    "\n",
    "\n",
    "# Standard deviation\n",
    "data['std5m'] = talib.STDDEV(data['close'], timeperiod=5)/ data['close'] \n",
    "data['std10m'] = talib.STDDEV(data['close'], timeperiod = 10) / data['close'] \n",
    "data['std1h'] = talib.STDDEV(data['close'], timeperiod = 60) / data['close'] \n",
    "data['std4h'] = talib.STDDEV(data['close'], timeperiod = 240) / data['close'] \n",
    "data['std12h'] = talib.STDDEV(data['close'], timeperiod = 720) / data['close'] \n",
    "data['std1d'] = talib.STDDEV(data['close'], timeperiod = 1440) / data['close']\n",
    "data['std5d'] = talib.STDDEV(data['close'], timeperiod = 7200) / data['close'] \n",
    "data['std10d'] = talib.STDDEV(data['close'], timeperiod = 14400) / data['close'] \n",
    "data['std30d'] = talib.STDDEV(data['close'], timeperiod = 43200) / data['close'] \n",
    "\n",
    "# Closeness to hundred / thousand\n",
    "data['dis100'] = (data['close'] % 100) / 100 \n",
    "data['dis1000'] = (data['close'] % 1000) / 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = ['ma5m','ma10m','ma1h','ma4h','ma12h','ma1d','ma5d','ma10d','ma30d', \\\n",
    "               'std5m','std10m','std1h','std4h','std12h','std1d','std5d','std10d','std30d',\\\n",
    "               'dis100', 'dis1000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment with same features in previous timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feature_set = []\n",
    "for feature in feature_set:\n",
    "    data[f'{feature}_p1h'] = data[feature].shift(60)\n",
    "    data[f'{feature}_p1d'] = data[feature].shift(1440)\n",
    "    data[f'{feature}_p5d'] = data[feature].shift(7200)\n",
    "    new_feature_set += [feature, f'{feature}_p1h', f'{feature}_p1d', f'{feature}_p5d']\n",
    "feature_set = new_feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total feature number 80\n"
     ]
    }
   ],
   "source": [
    "print(f'Total feature number {len(feature_set)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show distribution of return in 1d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9.00000e+00, 5.60000e+01, 3.20000e+02, 1.95000e+02, 5.63000e+02,\n",
       "        1.26600e+03, 3.88500e+03, 5.76200e+03, 1.46120e+04, 4.65010e+04,\n",
       "        1.37749e+05, 3.97512e+05, 2.54872e+05, 7.25690e+04, 2.87060e+04,\n",
       "        9.17900e+03, 6.52600e+03, 3.46900e+03, 1.22700e+03, 2.61000e+02,\n",
       "        2.04000e+02, 1.23000e+02, 1.93000e+02, 2.68000e+02, 1.59000e+02,\n",
       "        1.02000e+02, 9.70000e+01, 9.90000e+01, 6.60000e+01, 6.70000e+01,\n",
       "        4.30000e+01, 8.80000e+01, 5.00000e+01, 3.00000e+01, 2.20000e+01,\n",
       "        1.20000e+01, 1.00000e+01, 4.10000e+01, 4.10000e+01, 4.00000e+01,\n",
       "        1.10000e+01, 3.00000e+00, 2.00000e+00, 4.00000e+00, 1.20000e+01,\n",
       "        9.00000e+00, 3.00000e+00, 1.00000e+00, 3.00000e+00, 1.00000e+00]),\n",
       " array([-0.28686676, -0.26269117, -0.23851558, -0.21433999, -0.19016441,\n",
       "        -0.16598882, -0.14181323, -0.11763764, -0.09346206, -0.06928647,\n",
       "        -0.04511088, -0.0209353 ,  0.00324029,  0.02741588,  0.05159147,\n",
       "         0.07576705,  0.09994264,  0.12411823,  0.14829381,  0.1724694 ,\n",
       "         0.19664499,  0.22082058,  0.24499616,  0.26917175,  0.29334734,\n",
       "         0.31752293,  0.34169851,  0.3658741 ,  0.39004969,  0.41422527,\n",
       "         0.43840086,  0.46257645,  0.48675204,  0.51092762,  0.53510321,\n",
       "         0.5592788 ,  0.58345439,  0.60762997,  0.63180556,  0.65598115,\n",
       "         0.68015673,  0.70433232,  0.72850791,  0.7526835 ,  0.77685908,\n",
       "         0.80103467,  0.82521026,  0.84938585,  0.87356143,  0.89773702,\n",
       "         0.92191261]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data['close'] / data['close'].shift(-1440) - 1,bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['up_x%_in_1d_label'] = 100 * (data['close'] / data['close'].shift(-60) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_time = pd.Timestamp('2019-01-01', tz=TZ)\n",
    "train_end_time = pd.Timestamp('2020-4-1', tz=TZ)\n",
    "val_start_time = pd.Timestamp('2020-4-1', tz=TZ)\n",
    "val_end_time = pd.Timestamp('2020-08-01', tz=TZ)\n",
    "test_start_time = pd.Timestamp('2020-08-01', tz=TZ)\n",
    "test_end_time = pd.Timestamp('2020-12-31', tz=TZ)\n",
    "\n",
    "train_data = data.loc[train_start_time:train_end_time]\n",
    "val_data = data.loc[val_start_time:val_end_time]\n",
    "test_data = data.loc[test_start_time:test_end_time]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discritize continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data[feature_set].values.squeeze().astype(np.float32)\n",
    "X_val = val_data[feature_set].values.squeeze().astype(np.float32)\n",
    "X_test = test_data[feature_set].values.squeeze().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KBinsDiscretizer(encode='ordinal', strategy='uniform')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
    "est.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = est.transform(X_train)\n",
    "X_val = est.transform(X_val)\n",
    "X_test = est.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['up_x%_in_1d_label']\n",
    "\n",
    "positive_threshold = 1\n",
    "\n",
    "#Train\n",
    "y_train = (train_data[label].values > positive_threshold).astype(int)\n",
    "y_train_soft = np.zeros((len(y_train),2))\n",
    "y_train_soft[:,1] = np.clip(1 + train_data[label].values.squeeze() - positive_threshold, 0, 1)\n",
    "y_train_soft[:,0] = 1 - y_train_soft[:,1]\n",
    "\n",
    "#Val\n",
    "y_val = (val_data[label].values > positive_threshold).astype(int)\n",
    "y_val_soft = np.zeros((len(y_val),2))\n",
    "y_val_soft[:,1] = np.clip(1 + val_data[label].values.squeeze() - positive_threshold, 0, 1)\n",
    "y_val_soft[:,0] = 1 - y_val_soft[:,1]\n",
    "\n",
    "#Test\n",
    "y_test = (test_data[label].values > positive_threshold).astype(int)\n",
    "y_test_soft = np.zeros((len(y_test),2))\n",
    "y_test_soft[:,1] = np.clip(0.5 + (test_data[label].values.squeeze()-positive_threshold), 0, 1)\n",
    "y_test_soft[:,0] = 1 - y_test_soft[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive ratio in train set: 0.04893900042177017\n",
      "Positive ratio in val set: 0.032178778740390816\n",
      "Positive ratio in test set: 0.032124593728161156\n"
     ]
    }
   ],
   "source": [
    "print(f'Positive ratio in train set: {np.sum(y_train) / y_train.size}')\n",
    "print(f'Positive ratio in val set: {np.sum(y_val) / y_val.size}')\n",
    "print(f'Positive ratio in test set: {np.sum(y_test) / y_test.size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_from_fpr_tpr(fpr, tpr):\n",
    "    lw = 2\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    #plt.figure(figsize=(5,5))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='(AUC = %0.3f)' % (roc_auc))\n",
    "    #plt.plot([eer], [1-eer], marker='o', markersize=5, color=\"navy\")\n",
    "    #plt.plot([0, 1], [1, 0], color='navy', lw=1, linestyle=':')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic Curve')\n",
    "    plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, data, labels):\n",
    "        'Initialization'\n",
    "        self.data = torch.from_numpy(data)\n",
    "        self.labels = torch.from_numpy(labels.squeeze())\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        # Load data and get label\n",
    "        X = self.data[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_indexed(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, data, labels, ind):\n",
    "        'Initialization'\n",
    "        self.data = torch.from_numpy(data)\n",
    "        self.labels = torch.from_numpy(labels.squeeze())\n",
    "        self.ind = torch.from_numpy(ind.squeeze())\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        # Load data and get label\n",
    "        X = self.data[index]\n",
    "        y = self.labels[index]\n",
    "        i = self.ind[index]\n",
    "\n",
    "        return X, y, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 1024,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0,\n",
    "          'pin_memory': True,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Dataset(X_train, y_train)\n",
    "train_set_soft = Dataset(X_train, y_train_soft)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, **params)\n",
    "train_loader_soft = torch.utils.data.DataLoader(train_set_soft, **params)\n",
    "\n",
    "val_set = Dataset(X_val, y_val)\n",
    "val_set_soft = Dataset(X_val, y_val_soft)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, **params)\n",
    "val_loader_soft = torch.utils.data.DataLoader(val_set_soft, **params)\n",
    "\n",
    "test_set = Dataset(X_test, y_test)\n",
    "test_set_soft = Dataset(X_test, y_test_soft)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, **params)\n",
    "test_loader_soft = torch.utils.data.DataLoader(test_set_soft, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Model (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For reading, visualizing, and preprocessing data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define \"soft\" cross-entropy with pytorch tensor operations\n",
    "class softXEnt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(softXEnt, self).__init__()\n",
    "\n",
    "    def forward (self, input, target):\n",
    "        logprobs = torch.nn.functional.log_softmax (input, dim = 1)\n",
    "        return  -(target * logprobs).sum() / input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs, action_space, hidden_size1=40, hidden_size2=20):\n",
    "        super(Net, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        num_outputs = action_space\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size1)\n",
    "        self.linear2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.linear3 = nn.Linear(hidden_size2, num_outputs)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size1)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.bn1(F.relu(self.linear1(x)))\n",
    "        x = self.bn2(F.relu(self.linear2(x)))\n",
    "        out = self.linear3(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "def train_indexed(model, device, train_loader, criterion, optimizer, epoch, sample_weight):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target, index) in enumerate(train_loader):\n",
    "        data, target, index = data.to(device), target.to(device), index.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        # reduction = 'none', to add in SR\n",
    "        w = torch.from_numpy(sample_weight)\n",
    "        #print(loss.size())\n",
    "        #print(w[index].size())\n",
    "        loss = (loss * w[index]).sum()/w[index].sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    label_gt = []\n",
    "    label_pred = []\n",
    "    label_pred_score = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() * len(output)  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "\n",
    "            if len(target.shape) > 1:\n",
    "                target = target.argmax(dim=1, keepdim=False)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "            label_gt += list(target.cpu().numpy())\n",
    "            label_pred += list(pred.cpu().numpy())\n",
    "            label_pred_score += list(output[:,1].cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "        \n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(label_gt, label_pred_score)\n",
    "    roc_auc = auc(fpr, tpr) \n",
    "    \n",
    "    return (test_loss, accuracy, roc_auc, fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjyang/.local/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = 2\n",
    "max_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Sample Reweighting & Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sample_reweight import SampleReweight\n",
    "from featureSelection import featureSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMITED_SAMPLE = False\n",
    "EXAMPLES = 40000 if LIMITED_SAMPLE else X_train.shape[0]\n",
    "\n",
    "# number of submodels\n",
    "K = 5\n",
    "\n",
    "# initialize sample weights and parameters\n",
    "w = np.ones(X_train.shape[0])[:EXAMPLES]\n",
    "\n",
    "ALPHA_1 = 1\n",
    "ALPHA_2 = 1\n",
    "NUM_BINS = 10\n",
    "GAMMA = 0.9\n",
    "\n",
    "# initialize feature selection parameters\n",
    "NUM_BINS_FS = 5\n",
    "ratio = [0.95, 0.85, 0.75, 0.65, 0.55]\n",
    "feature_selected = np.arange(len(feature_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Hard Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b8b2fb938c4ad3a37e7f877af14478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-9625c8db27af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_roc_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_fpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tpr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_roc_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_fpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_tpr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mtrain_roc_auc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_roc_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mval_roc_auc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_roc_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-b5f9c68ffc60>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, device, test_loader, criterion)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_single = Net(input_dim, output_dim).to(device)\n",
    "optimizer = optim.AdamW(model_single.parameters(), lr=0.0006, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
    "#criterion = softXEnt().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "train_roc_auc_list = []\n",
    "val_roc_auc_list = []\n",
    "\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "    train(model_single, device, train_loader, criterion, optimizer, epoch)\n",
    "    (train_loss, train_accuracy, train_roc_auc, train_fpr, train_tpr) = test(model_single, device, train_loader, criterion)\n",
    "    (val_loss, val_accuracy, val_roc_auc, val_fpr, val_tpr) = test(model_single, device, val_loader, criterion)\n",
    "    train_roc_auc_list.append(train_roc_auc)\n",
    "    val_roc_auc_list.append(val_roc_auc)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_roc_auc_list)\n",
    "plt.plot(val_roc_auc_list)\n",
    "plt.title('ROC AUC')\n",
    "plt.legend(['train','val'])\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_roc_from_fpr_tpr(val_fpr, val_tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 submodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs_5_sub = []\n",
    "optmzs_5_sub = []\n",
    "schedulers_5_sub = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "for i in tqdm(range(K)):\n",
    "    # initiate submodel\n",
    "    clfs_5_sub.append(Net(input_dim, output_dim).to(device))\n",
    "    optmzs_5_sub.append(optim.AdamW(clfs_5_sub[i].parameters(), lr=0.0006, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False))\n",
    "    schedulers_5_sub.append(StepLR(optmzs_5_sub[i], step_size=1, gamma=0.7))\n",
    "    \n",
    "    train_set = Dataset(X_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, **params)\n",
    "    \n",
    "    for epoch in tqdm(range(max_epochs)):\n",
    "        train(clfs_5_sub[i], device, train_loader, criterion, optmzs_5_sub[i], epoch)\n",
    "        schedulers_5_sub[i].step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### +SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = np.arange(X_train.shape[0])\n",
    "train_set = Dataset_indexed(X_train, y_train, train_index)\n",
    "train_set_soft = Dataset_indexed(X_train, y_train_soft, train_index)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, **params)\n",
    "train_loader_soft = torch.utils.data.DataLoader(train_set_soft, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs_SR = []\n",
    "optmzs_SR = []\n",
    "schedulers_SR = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "for i in tqdm(range(K)):\n",
    "    # initiate submodel\n",
    "    clfs_SR.append(Net(input_dim, output_dim).to(device))\n",
    "    optmzs_SR.append(optim.AdamW(clfs_SR[i].parameters(), lr=0.0006, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False))\n",
    "    schedulers_SR.append(StepLR(optmzs_SR[i], step_size=1, gamma=0.7))\n",
    "    \n",
    "    staged_pred = []\n",
    "    train_set = Dataset_indexed(X_train, y_train, train_index)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, **params)\n",
    "    \n",
    "    for epoch in tqdm(range(max_epochs)):\n",
    "        train_indexed(clfs_SR[i], device, train_loader, criterion, optmzs_SR[i], epoch, sample_weight=w)\n",
    "        schedulers_SR[i].step()\n",
    "        with torch.no_grad():\n",
    "            X_pred = torch.from_numpy(X_train)\n",
    "            staged_pred.append((torch.nn.functional.log_softmax(clfs_SR[i](X_pred), dim = 1).numpy()))\n",
    "    # initialize sample reweighting\n",
    "    SR = SampleReweight(X_train, y_train, a1=ALPHA_1, a2=ALPHA_2, b=NUM_BINS, gamma=GAMMA)\n",
    "    w = SR.reweight(clfs_SR[i], staged_pred)\n",
    "    print(np.max(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### +SR+FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = np.arange(X_train.shape[0])\n",
    "train_set = Dataset_indexed(X_train, y_train, train_index)\n",
    "train_set_soft = Dataset_indexed(X_train, y_train_soft, train_index)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, **params)\n",
    "train_loader_soft = torch.utils.data.DataLoader(train_set_soft, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs_SR_FS = []\n",
    "optmzs_SR_FS = []\n",
    "schedulers_SR_FS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selected = np.arange(len(feature_set))\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "list_feature_selected = []\n",
    "list_feature_selected.append(feature_selected.copy())\n",
    "\n",
    "for i in tqdm(range(K)):\n",
    "    # initiate submodel\n",
    "    input_dim = feature_selected.shape[0]\n",
    "    clfs_SR_FS.append(Net(input_dim, output_dim).to(device))\n",
    "    optmzs_SR_FS.append(optim.AdamW(clfs_SR_FS[i].parameters(), lr=0.0006, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False))\n",
    "    schedulers_SR_FS.append(StepLR(optmzs_SR_FS[i], step_size=1, gamma=0.7))\n",
    "    \n",
    "    staged_pred = []\n",
    "    train_set = Dataset_indexed(X_train[:, feature_selected], y_train, train_index)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, **params)\n",
    "    \n",
    "    for epoch in tqdm(range(max_epochs)):\n",
    "        train_indexed(clfs_SR_FS[i], device, train_loader, criterion, optmzs_SR_FS[i], epoch, sample_weight=w)\n",
    "        schedulers_SR_FS[i].step()\n",
    "        with torch.no_grad():\n",
    "            X_pred = torch.from_numpy(X_train[:,feature_selected])\n",
    "            staged_pred.append((torch.nn.functional.log_softmax(clfs_SR_FS[i](X_pred), dim = 1).numpy()))\n",
    "    # initialize sample reweighting\n",
    "    SR = SampleReweight(X_train[:, feature_selected], y_train, a1=ALPHA_1, a2=ALPHA_2, b=NUM_BINS, gamma=GAMMA)\n",
    "    w = SR.reweight(clfs_SR_FS[i], staged_pred)\n",
    "    print(np.max(w))\n",
    "    \n",
    "    #feature selection\n",
    "    feature_selected_index = featureSelection(clfs_SR_FS[i], X_val[:, feature_selected], y_val, NUM_BINS_FS, ratio)\n",
    "    feature_selected = feature_selected[feature_selected_index]\n",
    "    print(feature_selected)\n",
    "    list_feature_selected.append(feature_selected.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = np.arange(X_train.shape[0])\n",
    "train_set = Dataset_indexed(X_train, y_train, train_index)\n",
    "train_set_soft = Dataset_indexed(X_train, y_train_soft, train_index)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, **params)\n",
    "train_loader_soft = torch.utils.data.DataLoader(train_set_soft, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs_TFS = []\n",
    "optmzs_TFS = []\n",
    "schedulers_TFS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selected = np.arange(len(feature_set))\n",
    "from utils import filter_by_corr\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "list_feature_selected = []\n",
    "list_feature_selected.append(feature_selected.copy())\n",
    "\n",
    "for i in tqdm(range(K)):\n",
    "    # initiate submodel\n",
    "    input_dim = feature_selected.shape[0]\n",
    "    clfs_TFS.append(Net(input_dim, output_dim).to(device))\n",
    "    optmzs_TFS.append(optim.AdamW(clfs_TFS[i].parameters(), lr=0.0006, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False))\n",
    "    schedulers_TFS.append(StepLR(optmzs_TFS[i], step_size=1, gamma=0.7))\n",
    "    \n",
    "    train_set = Dataset_indexed(X_train[:, feature_selected], y_train, train_index)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, **params)\n",
    "    \n",
    "    for epoch in tqdm(range(max_epochs)):\n",
    "        train_indexed(clfs_TFS[i], device, train_loader, criterion, optmzs_TFS[i], epoch, sample_weight=w)\n",
    "        schedulers_SR_TFS[i].step()\n",
    "    \n",
    "    #feature selection\n",
    "    feature_selected_index = filter_by_corr(X_train[:, feature_selected])\n",
    "    feature_selected = feature_selected[feature_selected_index]\n",
    "    print(feature_selected)\n",
    "    list_feature_selected.append(feature_selected.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = np.arange(X_train.shape[0])\n",
    "train_set = Dataset_indexed(X_train, y_train, train_index)\n",
    "train_set_soft = Dataset_indexed(X_train, y_train_soft, train_index)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, **params)\n",
    "train_loader_soft = torch.utils.data.DataLoader(train_set_soft, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs_FS = []\n",
    "optmzs_FS = []\n",
    "schedulers_FS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selected = np.arange(len(feature_set))\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "list_feature_selected = []\n",
    "list_feature_selected.append(feature_selected.copy())\n",
    "\n",
    "for i in tqdm(range(K)):\n",
    "    # initiate submodel\n",
    "    input_dim = feature_selected.shape[0]\n",
    "    clfs_FS.append(Net(input_dim, output_dim).to(device))\n",
    "    optmzs_FS.append(optim.AdamW(clfs_FS[i].parameters(), lr=0.0006, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False))\n",
    "    schedulers_FS.append(StepLR(optmzs_FS[i], step_size=1, gamma=0.7))\n",
    "    \n",
    "    train_set = Dataset_indexed(X_train[:, feature_selected], y_train, train_index)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, **params)\n",
    "    \n",
    "    for epoch in tqdm(range(max_epochs)):\n",
    "        train_indexed(clfs_FS[i], device, train_loader, criterion, optmzs_FS[i], epoch, sample_weight=w)\n",
    "        schedulers_FS[i].step()\n",
    "    \n",
    "    #feature selection\n",
    "    feature_selected_index = featureSelection(clfs_FS[i], X_val[:, feature_selected], y_val, NUM_BINS_FS, ratio)\n",
    "    feature_selected = feature_selected[feature_selected_index]\n",
    "    print(feature_selected)\n",
    "    list_feature_selected.append(feature_selected.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_list = []\n",
    "model_single.eval()\n",
    "with torch.no_grad():\n",
    "        X_test_tensor = torch.from_numpy(X_test)\n",
    "        #softmax = torch.nn.functional.softmax(model_single(X_test_tensor), dim = 1)\n",
    "        #decision_list.append(softmax.numpy()[:,1])\n",
    "        decision_list.append((model_single(X_test_tensor)).numpy()[:,1])\n",
    "print(len(decision_list))\n",
    "print(np.asarray(decision_list).shape)\n",
    "y_score_single = np.mean(np.asarray(decision_list), axis = 0)\n",
    "print(y_score_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_list = []\n",
    "for i in range(K):\n",
    "    clfs_5_sub[i].eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.from_numpy(X_test)\n",
    "        #softmax = torch.nn.functional.softmax(clfs_5_sub[i](X_test_tensor), dim = 1)\n",
    "        #decision_list.append(softmax.numpy()[:,1])\n",
    "        decision_list.append((clfs_5_sub[i](X_test_tensor)).numpy()[:,1])\n",
    "print(len(decision_list))\n",
    "print(np.asarray(decision_list).shape)\n",
    "y_score_5_sub = np.mean(np.asarray(decision_list), axis = 0)\n",
    "#y_score_5_sub = np.asarray(decision_list[4])\n",
    "print(y_score_5_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_list = []\n",
    "for i in range(K):\n",
    "    clfs_SR[i].eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.from_numpy(X_test)\n",
    "        #softmax = torch.nn.functional.softmax(clfs_SR[i](X_test_tensor), dim = 1)\n",
    "        #decision_list.append(softmax.numpy()[:,1])\n",
    "        decision_list.append((clfs_SR[i](X_test_tensor)).numpy()[:,1])\n",
    "print(len(decision_list))\n",
    "print(np.asarray(decision_list).shape)\n",
    "y_score_SR = np.mean(np.asarray(decision_list), axis = 0)\n",
    "#y_score_SR = np.asarray(decision_list[4])\n",
    "print(y_score_SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_list = []\n",
    "for i in range(K):\n",
    "    clfs_FS[i].eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.from_numpy(X_test[:,list_feature_selected[i]])\n",
    "        #softmax = torch.nn.functional.softmax(clfs_SR[i](X_test_tensor), dim = 1)\n",
    "        #decision_list.append(softmax.numpy()[:,1])\n",
    "        decision_list.append((clfs_FS[i](X_test_tensor)).numpy()[:,1])\n",
    "print(len(decision_list))\n",
    "print(np.asarray(decision_list).shape)\n",
    "#y_score_FS = np.mean(np.asarray(decision_list[2:4]), axis = 0)\n",
    "y_score_FS = np.asarray(decision_list[4])\n",
    "print(y_score_FS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_list = []\n",
    "for i in range(K):\n",
    "    clfs_TFS[i].eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.from_numpy(X_test[:,list_feature_selected[i]])\n",
    "        #softmax = torch.nn.functional.softmax(clfs_SR[i](X_test_tensor), dim = 1)\n",
    "        #decision_list.append(softmax.numpy()[:,1])\n",
    "        decision_list.append((clfs_TFS[i](X_test_tensor)).numpy()[:,1])\n",
    "print(len(decision_list))\n",
    "print(np.asarray(decision_list).shape)\n",
    "y_score_TFS = np.mean(np.asarray(decision_list), axis = 0)\n",
    "#y_score_FS = np.asarray(decision_list[4])\n",
    "print(y_score_TFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_list = []\n",
    "for i in range(K):\n",
    "    clfs_SR_FS[i].eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.from_numpy(X_test[:,list_feature_selected[i]])\n",
    "        #softmax = torch.nn.functional.softmax(clfs_SR_FS[i](X_test_tensor), dim = 1)\n",
    "        #decision_list.append(softmax.numpy()[:,1])\n",
    "        decision_list.append((clfs_SR_FS[i](X_test_tensor)).numpy()[:,1])\n",
    "print(len(decision_list))\n",
    "print(np.asarray(decision_list).shape)\n",
    "y_score_SR_FS = np.mean(np.asarray(decision_list), axis = 0)\n",
    "#y_score_SR_FS = np.asarray(decision_list[4])\n",
    "print(y_score_SR_FS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(labels, scores):\n",
    "    lw = 2\n",
    "    fpr, tpr, _ = roc_curve(labels, scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='(AUC = %0.3f)' % (roc_auc))\n",
    "    #plt.plot([eer], [1-eer], marker='o', markersize=5, color=\"navy\")\n",
    "    #plt.plot([0, 1], [1, 0], color='navy', lw=1, linestyle=':')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic Curve')\n",
    "    plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(y_test, y_score_single)\n",
    "plot_roc(y_test, y_score_5_sub)\n",
    "plot_roc(y_test, y_score_SR)\n",
    "plot_roc(y_test, y_score_SR_FS)\n",
    "plot_roc(y_test, y_score_FS)\n",
    "plot_roc(y_test, y_score_TFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
