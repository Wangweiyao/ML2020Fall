{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "Here we prepare data as 1 min level kline for BTC from 2019.1.1 to 2020.5.2 in bitfinex exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m pip install -e .. -U\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/data'\n",
    "data_platform_list = ['BITFINEX']\n",
    "data_symbol_list = ['BTC']\n",
    "\n",
    "data_df_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for platform in data_platform_list:\n",
    "    for symbol in data_symbol_list:\n",
    "        pkl_file_path = data_path+'/'+symbol+'_USD_'+platform+'_latest.pkl'\n",
    "        pandas_df = pd.read_pickle(pkl_file_path)\n",
    "        #data_df_list.append(pkl_file.add_prefix(platform+'_'+symbol+':'))\n",
    "        data_df_list.append(pandas_df)\n",
    "data = pd.concat(data_df_list, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Enginnering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tactical indicators etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['timestamp'] = data.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "import talib\n",
    "\n",
    "# Moving averages\n",
    "data['ma5m'] = talib.MA(data['close'], timeperiod = 5) / data['close'] \n",
    "data['ma10m'] = talib.MA(data['close'], timeperiod = 10) / data['close'] \n",
    "data['ma1h'] = talib.MA(data['close'], timeperiod = 60) / data['close'] \n",
    "data['ma4h'] = talib.MA(data['close'], timeperiod = 240) / data['close'] \n",
    "data['ma12h'] = talib.MA(data['close'], timeperiod = 720) / data['close'] \n",
    "data['ma1d'] = talib.MA(data['close'], timeperiod = 1440) / data['close']\n",
    "data['ma5d'] = talib.MA(data['close'], timeperiod = 7200) / data['close'] \n",
    "data['ma10d'] = talib.MA(data['close'], timeperiod = 14400) / data['close'] \n",
    "data['ma30d'] = talib.MA(data['close'], timeperiod = 43200) / data['close'] \n",
    "\n",
    "\n",
    "# Standard deviation\n",
    "data['std5m'] = talib.STDDEV(data['close'], timeperiod=5)/ data['close'] \n",
    "data['std10m'] = talib.STDDEV(data['close'], timeperiod = 10) / data['close'] \n",
    "data['std1h'] = talib.STDDEV(data['close'], timeperiod = 60) / data['close'] \n",
    "data['std4h'] = talib.STDDEV(data['close'], timeperiod = 240) / data['close'] \n",
    "data['std12h'] = talib.STDDEV(data['close'], timeperiod = 720) / data['close'] \n",
    "data['std1d'] = talib.STDDEV(data['close'], timeperiod = 1440) / data['close']\n",
    "data['std5d'] = talib.STDDEV(data['close'], timeperiod = 7200) / data['close'] \n",
    "data['std10d'] = talib.STDDEV(data['close'], timeperiod = 14400) / data['close'] \n",
    "data['std30d'] = talib.STDDEV(data['close'], timeperiod = 43200) / data['close'] \n",
    "\n",
    "'''\n",
    "# Linear regression predict fit\n",
    "data['reg5m'] = talib.LINEARREG(data['close'], timeperiod=5)\n",
    "data['reg10m'] = talib.LINEARREG(data['close'], timeperiod = 10) \n",
    "data['reg1h'] = talib.LINEARREG(data['close'], timeperiod = 60) \n",
    "data['reg4h'] = talib.LINEARREG(data['close'], timeperiod = 240) \n",
    "data['reg12h'] = talib.LINEARREG(data['close'], timeperiod = 720) \n",
    "data['reg1d'] = talib.LINEARREG(data['close'], timeperiod = 1440) \n",
    "data['reg5d'] = talib.LINEARREG(data['close'], timeperiod = 7200)\n",
    "data['reg10d'] = talib.LINEARREG(data['close'], timeperiod = 14400) \n",
    "data['reg30d'] = talib.LINEARREG(data['close'], timeperiod = 43200) \n",
    "\n",
    "\n",
    "# Linear regression slope \n",
    "data['slope5m'] = talib.LINEARREG_SLOPE(data['close'], timeperiod=5)\n",
    "data['slope10m'] = talib.LINEARREG_SLOPE(data['close'], timeperiod = 10) \n",
    "data['slope1h'] = talib.LINEARREG_SLOPE(data['close'], timeperiod = 60) \n",
    "data['slope4h'] = talib.LINEARREG_SLOPE(data['close'], timeperiod = 240) \n",
    "data['slope12h'] = talib.LINEARREG_SLOPE(data['close'], timeperiod = 720)\n",
    "data['slope1d'] = talib.LINEARREG_SLOPE(data['close'], timeperiod = 1440) \n",
    "data['slope5d'] = talib.LINEARREG_SLOPE(data['close'], timeperiod = 7200) \n",
    "data['slope10d'] = talib.LINEARREG_SLOPE(data['close'], timeperiod = 14400) \n",
    "data['slope30d'] = talib.LINEARREG_SLOPE(data['close'], timeperiod = 43200) \n",
    "'''\n",
    "\n",
    "# Closeness to hundred / thousand\n",
    "data['dis100'] = (data['close'] % 100) / 100 \n",
    "data['dis1000'] = (data['close'] % 1000) / 1000 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Strategy Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nextrade.data import Node, Module, DataFeed, Stream, Select\n",
    "\n",
    "def preprocess(data: pd.DataFrame):\n",
    "    features = []    \n",
    "    features += [Stream(list(data.index)).rename(\"Timestamp\")]\n",
    "    for c in data.columns:\n",
    "        s = Stream(list(data[c])).rename(data[c].name)\n",
    "        features += [s]\n",
    "        \n",
    "    feed = DataFeed(features)\n",
    "    feed.compile()\n",
    "    return feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start_time = pd.to_datetime('2019-01-01').date()\n",
    "test_end_time = pd.to_datetime('2020-09-12').date()\n",
    "test_data = data.loc[test_start_time:test_end_time]\n",
    "\n",
    "step_size = 240 # data frequency\n",
    "test_data = test_data.iloc[::step_size, :]\n",
    "\n",
    "from nextrade.actions.simple_orders import SimpleOrders\n",
    "test_env_config = {'data':test_data, \n",
    "                    'start_time':test_start_time, \n",
    "                    'end_time':test_end_time, \n",
    "                    'preprocess':preprocess, \n",
    "                    'rl_env':False, \n",
    "                    'slice_len':-1,\n",
    "                    'use_internal':True,\n",
    "                    'exchange_name':'BITF',\n",
    "                    'renders':['plotly'],\n",
    "                    'BTC_SWAP_MUL':10}\n",
    "\n",
    "from nextrade.environments import TradingEnvironment\n",
    "from nextrade.environments import SwapTradingEnvironment\n",
    "test_env = TradingEnvironment(test_env_config)\n",
    "test_swap_env = SwapTradingEnvironment(test_env_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2_curve(name1, data1, name2, data2, t):\n",
    "# Create some mock data\n",
    "    fig, ax1 = plt.subplots(figsize=(15,5))\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('time')\n",
    "    ax1.set_ylabel(name1, color=color)\n",
    "    ax1.plot(t, data1, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel(name2, color=color)  # we already handled the x-label with ax1\n",
    "    ax2.plot(t, data2, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata1 = talib.STDDEV(data[\"reg1d\"]-data[\"close\"],timeperiod = 60*24*1) / data[\"close\"]\\ndata2 = data[\"close\"]\\nplot_2_curve(name1=\"reg\",data1=data1, name2=\"close\", data2=data2, t=data.timestamp)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data1 = talib.STDDEV(data[\"reg1d\"]-data[\"close\"],timeperiod = 60*24*1) / data[\"close\"]\n",
    "data2 = data[\"close\"]\n",
    "plot_2_curve(name1=\"reg\",data1=data1, name2=\"close\", data2=data2, t=data.timestamp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nextrade.orders import TradeSide\n",
    "LONG_OPEN = TradeSide.LONG_OPEN\n",
    "SHORT_OPEN = TradeSide.SHORT_OPEN\n",
    "LONG_CLOSE = TradeSide.LONG_CLOSE\n",
    "SHORT_CLOSE = TradeSide.SHORT_CLOSE\n",
    "\n",
    "class Strategy():\n",
    "    def __init__(self, feature_set, step_size):\n",
    "        self.feature_set = feature_set\n",
    "        self.ma1 = feature_set[0]\n",
    "        self.ma2 = feature_set[1]\n",
    "        self.step_size = step_size\n",
    "        self.sleep_after_close_hr = 24\n",
    "        \n",
    "        self.last_close_timestamp_1 = None\n",
    "        self.last_close_timestamp_2 = None\n",
    "        \n",
    "    def get_action(self, obs):\n",
    "        \n",
    "        if self.last_close_timestamp_1 != None:\n",
    "            time_past = obs['timestamp'] - self.last_close_timestamp_1\n",
    "            if obs[\"std5d\"]/obs[\"ma5d\"] > 0.05:\n",
    "                action = 4 # too close to last close, dont act\n",
    "                self.last_close_timestamp_1 = None\n",
    "                return action\n",
    "            \n",
    "        if self.last_close_timestamp_2 != None:\n",
    "            time_past = obs['timestamp'] - self.last_close_timestamp_2\n",
    "            if time_past < timedelta(hours=72) and obs[\"std5d\"]/obs[\"ma5d\"] > 0.025:\n",
    "                action = 4 # too close to last close, dont act\n",
    "                self.last_close_timestamp_2 = None\n",
    "                return action\n",
    "            \n",
    "        if obs['ma5m'] / obs['ma10d'] > 1.2:\n",
    "            action = 4\n",
    "            self.last_close_timestamp_1 = obs['timestamp']\n",
    "            return action # it goes up much, there will probably be reversion\n",
    "        \n",
    "        if obs[self.ma1] > obs[self.ma2]:\n",
    "            action = 3\n",
    "        else:\n",
    "            action = 4\n",
    "            self.last_close_timestamp_2 = obs['timestamp']\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_metrics = ['net_worth','BITF:/USD-BTC','BITF:/BTC:/worth']\n",
    "plt_df = env.portfolio.performance[plt_metrics].copy()\n",
    "plt_df['BITF:/BTC:/worth'] = plt_df['BITF:/BTC:/worth'] / plt_df['net_worth'].iloc[0]\n",
    "plt_df['net_worth'] = plt_df['net_worth'] / plt_df['net_worth'].iloc[0]\n",
    "plt_df['BITF:/USD-BTC'] = plt_df['BITF:/USD-BTC'] / plt_df['BITF:/USD-BTC'].iloc[0]\n",
    "plt_df.columns = [\"portfolio value\", \"asset price\", \"asset position\"]\n",
    "plt_df.plot(figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Strategy Example\n",
    "\n",
    "refernces: https://towardsdatascience.com/model-design-and-selection-with-scikit-learn-18a29041d02a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_time = pd.to_datetime('2020-01-01').date()\n",
    "train_end_time = pd.to_datetime('2020-02-15').date()\n",
    "test_start_time = pd.to_datetime('2020-07-15').date()\n",
    "test_end_time = pd.to_datetime('2020-08-15').date()\n",
    "\n",
    "train_data = data.loc[train_start_time:train_end_time]\n",
    "test_data = data.loc[test_start_time:test_end_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = ['ma5m','ma10m','ma1h','ma4h','ma12h','ma1d','ma5d', \\\n",
    "               'ma10m-5m', 'ma1h-10m', 'ma4h-1h', 'ma12h-4h', 'ma1d-12h', 'ma5d-1d', \\\n",
    "               'natr5m','natr10m','natr1h','natr4h','natr12h','natr1d','natr5d', \\\n",
    "               'adosc5m-10m', 'adosc10m-1h', 'adosc1h-4h', 'adosc4h-12h', 'adosc12h-1d', 'adosc1d-5d', \\\n",
    "               'rsi5m','rsi10m','rsi1h','rsi4h','rsi12h','rsi1d','rsi5d', \\\n",
    "               'dis100', 'dis1000']\n",
    "label = ['label_2.0']\n",
    "\n",
    "X_train = train_data[feature_set]\n",
    "y_train = train_data[label]\n",
    "X_test = test_data[feature_set]\n",
    "y_test = test_data[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model and hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reading, visualizing, and preprocessing data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Classifiers\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "\n",
    "xgboost_model = XGBClassifier(learning_rate=0.01,\n",
    "                      n_estimators=10,           # 树的个数-10棵树建立xgboost\n",
    "                      max_depth=4,               # 树的深度\n",
    "                      min_child_weight = 1,      # 叶子节点最小权重\n",
    "                      gamma=0.,                  # 惩罚项中叶子结点个数前的参数\n",
    "                      subsample=1,               # 所有样本建立决策树\n",
    "                      colsample_btree=1,         # 所有特征建立决策树\n",
    "                      scale_pos_weight=1,        # 解决样本个数不平衡的问题\n",
    "                      random_state=27,           # 随机数\n",
    "                      slient = 0\n",
    "                      )\n",
    "\n",
    "#                                  Classifiers                                #\n",
    "###############################################################################\n",
    "# Create list of tuples with classifier label and classifier object\n",
    "classifiers = {}\n",
    "classifiers.update({\"Random Forest\": RandomForestClassifier()})\n",
    "classifiers.update({\"LDA\": LinearDiscriminantAnalysis()})\n",
    "classifiers.update({\"QDA\": QuadraticDiscriminantAnalysis()})\n",
    "'''\n",
    "classifiers.update({\"AdaBoost\": AdaBoostClassifier()})\n",
    "classifiers.update({\"Bagging\": BaggingClassifier()})\n",
    "classifiers.update({\"Extra Trees Ensemble\": ExtraTreesClassifier()})\n",
    "classifiers.update({\"Gradient Boosting\": GradientBoostingClassifier()})\n",
    "classifiers.update({\"Ridge\": RidgeClassifier()})\n",
    "classifiers.update({\"SGD\": SGDClassifier()})\n",
    "classifiers.update({\"BNB\": BernoulliNB()})\n",
    "classifiers.update({\"GNB\": GaussianNB()})\n",
    "classifiers.update({\"KNN\": KNeighborsClassifier()})\n",
    "classifiers.update({\"LSVC\": LinearSVC()})\n",
    "classifiers.update({\"NuSVC\": NuSVC()})\n",
    "classifiers.update({\"SVC\": SVC()})\n",
    "classifiers.update({\"DTC\": DecisionTreeClassifier()})\n",
    "classifiers.update({\"ETC\": ExtraTreeClassifier()})\n",
    "'''\n",
    "\n",
    "# Create dict of decision function labels\n",
    "DECISION_FUNCTIONS = {\"Ridge\", \"SGD\", \"LSVC\", \"NuSVC\", \"SVC\"}\n",
    "\n",
    "# Create dict for classifiers with feature_importances_ attribute\n",
    "FEATURE_IMPORTANCE = {\"Gradient Boosting\", \"Extra Trees Ensemble\", \"Random Forest\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#                                Hyper-parameters                             #\n",
    "###############################################################################\n",
    "# Initiate parameter grid\n",
    "parameters = {}\n",
    "\n",
    "# Update dict with LDA\n",
    "parameters.update({\"LDA\": {\"classifier__solver\": [\"svd\"], \n",
    "                                         }})\n",
    "\n",
    "# Update dict with QDA\n",
    "parameters.update({\"QDA\": {\"classifier__reg_param\":[0.01*ii for ii in range(0, 101)], \n",
    "                                         }})\n",
    "\n",
    "# Update dict with Random Forest Parameters\n",
    "parameters.update({\"Random Forest\": { \n",
    "                                    \"classifier__n_estimators\": [200],\n",
    "                                    \"classifier__class_weight\": [None, \"balanced\"],\n",
    "                                    \"classifier__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "                                    \"classifier__max_depth\" : [4, 8],\n",
    "                                    \"classifier__min_samples_split\": [0.01, 0.10],\n",
    "                                    \"classifier__min_samples_leaf\": [0.01, 0.10],\n",
    "                                    \"classifier__criterion\" :[\"gini\", \"entropy\"]     ,\n",
    "                                    \"classifier__n_jobs\": [-1]\n",
    "                                     }})\n",
    "'''\n",
    "# Update dict with AdaBoost\n",
    "parameters.update({\"AdaBoost\": { \n",
    "                                \"classifier__base_estimator\": [DecisionTreeClassifier(max_depth = ii) for ii in range(1,6)],\n",
    "                                \"classifier__n_estimators\": [200],\n",
    "                                \"classifier__learning_rate\": [0.001, 0.01, 0.1, 0.50, 1.0]\n",
    "                                 }})\n",
    "\n",
    "# Update dict with Bagging\n",
    "parameters.update({\"Bagging\": { \n",
    "                                \"classifier__base_estimator\": [DecisionTreeClassifier(max_depth = ii) for ii in range(1,6)],\n",
    "                                \"classifier__n_estimators\": [200],\n",
    "                                \"classifier__max_features\": [0.2, 0.4, 0.8],\n",
    "                                \"classifier__n_jobs\": [-1]\n",
    "                                }})\n",
    "\n",
    "# Update dict with Gradient Boosting\n",
    "parameters.update({\"Gradient Boosting\": { \n",
    "                                        \"classifier__learning_rate\":[0.1,0.01,0.001], \n",
    "                                        \"classifier__n_estimators\": [200],\n",
    "                                        \"classifier__max_depth\": [2,4,8],\n",
    "                                        \"classifier__min_samples_split\": [0.01, 0.10],\n",
    "                                        \"classifier__min_samples_leaf\": [0.01, 0.10],\n",
    "                                        \"classifier__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "                                        \"classifier__subsample\": [0.8, 1]\n",
    "                                         }})\n",
    "\n",
    "\n",
    "# Update dict with Extra Trees\n",
    "parameters.update({\"Extra Trees Ensemble\": { \n",
    "                                            \"classifier__n_estimators\": [200],\n",
    "                                            \"classifier__class_weight\": [None, \"balanced\"],\n",
    "                                            \"classifier__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "                                            \"classifier__max_depth\" : [2, 4, 8],\n",
    "                                            \"classifier__min_samples_split\": [0.01, 0.05, 0.10],\n",
    "                                            \"classifier__min_samples_leaf\": [0.01, 0.05, 0.10],\n",
    "                                            \"classifier__criterion\" :[\"gini\", \"entropy\"]     ,\n",
    "                                            \"classifier__n_jobs\": [-1]\n",
    "                                             }})\n",
    "                                             \n",
    "\n",
    "# Update dict with Ridge\n",
    "parameters.update({\"Ridge\": { \n",
    "                            \"classifier__alpha\": [1e-7, 1e-5, 1e-3, 1e-1, 0.25, 0.50, 0.75, 1.0]\n",
    "                             }})\n",
    "\n",
    "# Update dict with SGD Classifier\n",
    "parameters.update({\"SGD\": { \n",
    "                            \"classifier__alpha\": [1e-7, 1e-5, 1e-3, 1e-1, 0.25, 0.50, 0.75, 1.0],\n",
    "                            \"classifier__penalty\": [\"l1\", \"l2\"],\n",
    "                            \"classifier__n_jobs\": [-1]\n",
    "                             }})\n",
    "\n",
    "\n",
    "# Update dict with BernoulliNB Classifier\n",
    "parameters.update({\"BNB\": { \n",
    "                            \"classifier__alpha\": [1e-7,  1e-5, 1e-3, 1e-1, 0.25, 0.50, 0.75, 1.0]\n",
    "                             }})\n",
    "\n",
    "# Update dict with GaussianNB Classifier\n",
    "parameters.update({\"GNB\": { \n",
    "                            \"classifier__var_smoothing\": [1e-9, 1e-8,1e-7, 1e-6, 1e-5]\n",
    "                             }})\n",
    "\n",
    "# Update dict with K Nearest Neighbors Classifier\n",
    "parameters.update({\"KNN\": { \n",
    "                            \"classifier__n_neighbors\": list(range(1,10)),\n",
    "                            \"classifier__p\": [1, 2, 4],\n",
    "                            \"classifier__leaf_size\": [10, 20, 50],\n",
    "                            \"classifier__n_jobs\": [-1]\n",
    "                             }})\n",
    "\n",
    "parameters.update({\"LSVC\": { \n",
    "                            \"classifier__penalty\": [\"l2\"],\n",
    "                            \"classifier__C\": [0.001, 0.01, 0.1, 1.0, 10, 100]\n",
    "                             }})\n",
    "\n",
    "parameters.update({\"NuSVC\": { \n",
    "                            \"classifier__nu\": [0.25, 0.50, 0.75],\n",
    "                            \"classifier__kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
    "                            \"classifier__degree\": [1,2,4],\n",
    "                             }})\n",
    "\n",
    "parameters.update({\"SVC\": { \n",
    "                            \"classifier__kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
    "                            \"classifier__gamma\": [\"auto\"],\n",
    "                            \"classifier__C\": [0.1, 1, 10, 100],\n",
    "                            \"classifier__degree\": [1, 2, 4]\n",
    "                             }})\n",
    "\n",
    "\n",
    "# Update dict with Decision Tree Classifier\n",
    "parameters.update({\"DTC\": { \n",
    "                            \"classifier__criterion\" :[\"gini\", \"entropy\"],\n",
    "                            \"classifier__splitter\": [\"best\", \"random\"],\n",
    "                            \"classifier__class_weight\": [None, \"balanced\"],\n",
    "                            \"classifier__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "                            \"classifier__max_depth\" : [2, 4, 8],\n",
    "                            \"classifier__min_samples_split\": [0.01, 0.05, 0.10],\n",
    "                            \"classifier__min_samples_leaf\": [0.01, 0.05, 0.10],\n",
    "                             }})\n",
    "\n",
    "# Update dict with Extra Tree Classifier\n",
    "parameters.update({\"ETC\": { \n",
    "                            \"classifier__criterion\" :[\"gini\", \"entropy\"],\n",
    "                            \"classifier__splitter\": [\"best\", \"random\"],\n",
    "                            \"classifier__class_weight\": [None, \"balanced\"],\n",
    "                            \"classifier__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "                            \"classifier__max_depth\" : [2, 4, 8],\n",
    "                            \"classifier__min_samples_split\": [0.01, 0.05, 0.10],\n",
    "                            \"classifier__min_samples_leaf\": [0.01, 0.05, 0.10],\n",
    "                             }})\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Drop correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Filter Method: Spearman's Cross Correlation > 0.95\n",
    "# Make correlation matrix\n",
    "corr_matrix = X_train.corr(method = \"spearman\").abs()\n",
    "\n",
    "# Draw the heatmap\n",
    "sns.set(font_scale = 1.0)\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "sns.heatmap(corr_matrix, cmap= \"YlGnBu\", square=True, ax = ax)\n",
    "f.tight_layout()\n",
    "#plt.imshow()#\"correlation_matrix.png\", dpi = 1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Select upper triangle of matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "# Drop features\n",
    "X_train = X_train.drop(to_drop, axis = 1)\n",
    "X_test = X_test.drop(to_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive feature selection with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict for classifiers with feature_importances_ attribute\n",
    "FEATURE_IMPORTANCE = {\"Gradient Boosting\", \"Extra Trees Ensemble\", \"Random Forest\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune a good model with best hyperparameter to begin with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                        Tuning a classifier to use with RFECV                #\n",
    "###############################################################################\n",
    "# Define classifier to use as the base of the recursive feature elimination algorithm\n",
    "selected_classifier = \"Random Forest\"\n",
    "classifier = classifiers[selected_classifier]\n",
    "\n",
    "# Tune classifier (Took = 4.8 minutes)\n",
    "    \n",
    "# Scale features via Z-score normalization\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define steps in pipeline\n",
    "steps = [(\"scaler\", scaler), (\"classifier\", classifier)]\n",
    "\n",
    "# Initialize Pipeline object\n",
    "pipeline = Pipeline(steps = steps)\n",
    "  \n",
    "# Define parameter grid\n",
    "param_grid = parameters[selected_classifier]\n",
    "\n",
    "# Initialize GridSearch object\n",
    "gscv = GridSearchCV(pipeline, param_grid, cv = 3,  n_jobs= -1, verbose = 1, scoring = \"roc_auc\")\n",
    "                  \n",
    "# Fit gscv\n",
    "print(f\"Now tuning {selected_classifier}. Go grab a beer or something.\")\n",
    "gscv.fit(X_train, np.ravel(y_train))  \n",
    "\n",
    "# Get best parameters and score\n",
    "best_params = gscv.best_params_\n",
    "best_score = gscv.best_score_\n",
    "        \n",
    "# Update classifier parameters\n",
    "tuned_params = {item[12:]: best_params[item] for item in best_params}\n",
    "classifier.set_params(**tuned_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Recursively select features with cross validation using that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#                     Custom pipeline object to use with RFECV                #\n",
    "###############################################################################\n",
    "# Select Features using RFECV\n",
    "class PipelineRFE(Pipeline):\n",
    "    # Source: https://ramhiser.com/post/2018-03-25-feature-selection-with-scikit-learn-pipeline/\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        super(PipelineRFE, self).fit(X, y, **fit_params)\n",
    "        self.feature_importances_ = self.steps[-1][-1].feature_importances_\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#      Feature Selection: Recursive Feature Selection with Cross Validation   #\n",
    "###############################################################################\n",
    "# Define pipeline for RFECV\n",
    "steps = [(\"scaler\", scaler), (\"classifier\", classifier)]\n",
    "pipe = PipelineRFE(steps = steps)\n",
    "\n",
    "# Initialize RFECV object\n",
    "feature_selector = RFECV(pipe, cv = 3, step = 1, scoring = \"roc_auc\", verbose = 1)\n",
    "\n",
    "# Fit RFECV\n",
    "feature_selector.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Get selected features\n",
    "feature_names = X_train.columns\n",
    "selected_features = feature_names[feature_selector.support_].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Display how feature set influence performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#                                 Performance Curve                           #\n",
    "###############################################################################\n",
    "# Get Performance Data\n",
    "performance_curve = {\"Number of Features\": list(range(1, len(feature_names) + 1)),\n",
    "                    \"AUC\": feature_selector.grid_scores_}\n",
    "performance_curve = pd.DataFrame(performance_curve)\n",
    "\n",
    "# Performance vs Number of Features\n",
    "# Set graph style\n",
    "sns.set(font_scale = 1.75)\n",
    "sns.set_style({\"axes.facecolor\": \"1.0\", \"axes.edgecolor\": \"0.85\", \"grid.color\": \"0.85\",\n",
    "               \"grid.linestyle\": \"-\", 'axes.labelcolor': '0.4', \"xtick.color\": \"0.4\",\n",
    "               'ytick.color': '0.4'})\n",
    "colors = sns.color_palette(\"RdYlGn\", 20)\n",
    "line_color = colors[3]\n",
    "marker_colors = colors[-1]\n",
    "\n",
    "# Plot\n",
    "f, ax = plt.subplots(figsize=(13, 6.5))\n",
    "sns.lineplot(x = \"Number of Features\", y = \"AUC\", data = performance_curve,\n",
    "             color = line_color, lw = 4, ax = ax)\n",
    "sns.regplot(x = performance_curve[\"Number of Features\"], y = performance_curve[\"AUC\"],\n",
    "            color = marker_colors, fit_reg = False, scatter_kws = {\"s\": 200}, ax = ax)\n",
    "\n",
    "# Axes limits\n",
    "plt.xlim(0.5, len(feature_names)+0.5)\n",
    "plt.ylim(0.0, 0.925)\n",
    "\n",
    "# Generate a bolded horizontal line at y = 0\n",
    "ax.axhline(y = 0.625, color = 'black', linewidth = 1.3, alpha = .7)\n",
    "\n",
    "# Turn frame off\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "# Tight layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursively select a given number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                    Feature Selection: Recursive Feature Selection           #\n",
    "###############################################################################\n",
    "# Define pipeline for RFECV\n",
    "steps = [(\"scaler\", scaler), (\"classifier\", classifier)]\n",
    "pipe = PipelineRFE(steps = steps)\n",
    "\n",
    "# Initialize RFE object\n",
    "feature_selector = RFE(pipe, n_features_to_select = 16, step = 1, verbose = 1)\n",
    "\n",
    "# Fit RFE\n",
    "feature_selector.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Get selected features labels\n",
    "feature_names = X_train.columns\n",
    "selected_features = feature_names[feature_selector.support_].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display selected feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                      Visualizing Selected Features Importance               #\n",
    "###############################################################################\n",
    "# Get selected features data set\n",
    "X_train = X_train[selected_features]\n",
    "X_test = X_test[selected_features]\n",
    "\n",
    "# Train classifier\n",
    "classifier.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame(selected_features, columns = [\"Feature Label\"])\n",
    "feature_importance[\"Feature Importance\"] = classifier.feature_importances_\n",
    "\n",
    "# Sort by feature importance\n",
    "feature_importance = feature_importance.sort_values(by=\"Feature Importance\", ascending=False)\n",
    "\n",
    "# Set graph style\n",
    "sns.set(font_scale = 1.75)\n",
    "sns.set_style({\"axes.facecolor\": \"1.0\", \"axes.edgecolor\": \"0.85\", \"grid.color\": \"0.85\",\n",
    "               \"grid.linestyle\": \"-\", 'axes.labelcolor': '0.4', \"xtick.color\": \"0.4\",\n",
    "               'ytick.color': '0.4'})\n",
    "\n",
    "# Set figure size and create barplot\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.barplot(x = \"Feature Importance\", y = \"Feature Label\",\n",
    "            palette = reversed(sns.color_palette('YlOrRd', 15)),  data = feature_importance)\n",
    "\n",
    "# Generate a bolded horizontal line at y = 0\n",
    "ax.axvline(x = 0, color = 'black', linewidth = 4, alpha = .7)\n",
    "\n",
    "# Turn frame off\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "# Tight layout\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit all classifiers with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                           Classifier Tuning and Evaluation                  #\n",
    "###############################################################################\n",
    "# Initialize dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Tune and evaluate classifiers\n",
    "for classifier_label, classifier in classifiers.items():\n",
    "    # Print message to user\n",
    "    print(f\"Now tuning {classifier_label}.\")\n",
    "    \n",
    "    # Scale features via Z-score normalization\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Define steps in pipeline\n",
    "    steps = [(\"scaler\", scaler), (\"classifier\", classifier)]\n",
    "    \n",
    "    # Initialize Pipeline object\n",
    "    pipeline = Pipeline(steps = steps)\n",
    "      \n",
    "    # Define parameter grid\n",
    "    param_grid = parameters[classifier_label]\n",
    "    \n",
    "    # Initialize GridSearch object\n",
    "    gscv = GridSearchCV(pipeline, param_grid, cv = 3,  n_jobs= -1, verbose = 1, scoring = \"roc_auc\")\n",
    "                      \n",
    "    # Fit gscv\n",
    "    gscv.fit(X_train, np.ravel(y_train))  \n",
    "    \n",
    "    # Get best parameters and score\n",
    "    best_params = gscv.best_params_\n",
    "    best_score = gscv.best_score_\n",
    "    \n",
    "    # Update classifier parameters and define new pipeline with tuned classifier\n",
    "    tuned_params = {item[12:]: best_params[item] for item in best_params}\n",
    "    classifier.set_params(**tuned_params)\n",
    "            \n",
    "    # Make predictions\n",
    "    if classifier_label in DECISION_FUNCTIONS:\n",
    "        y_pred = gscv.decision_function(X_test)\n",
    "    else:\n",
    "        y_pred = gscv.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Evaluate model\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    # Save results\n",
    "    result = {\"Classifier\": gscv,\n",
    "              \"Best Parameters\": best_params,\n",
    "              \"Training AUC\": best_score,\n",
    "              \"Test AUC\": auc}\n",
    "    \n",
    "    results.update({classifier_label: result})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                  Visualing Results                          #\n",
    "###############################################################################\n",
    "# Initialize auc_score dictionary\n",
    "auc_scores = {\n",
    "              \"Classifier\": [],\n",
    "              \"AUC\": [],\n",
    "              \"AUC Type\": []\n",
    "              }\n",
    "\n",
    "# Get AUC scores into dictionary\n",
    "for classifier_label in results:\n",
    "    auc_scores.update({\"Classifier\": [classifier_label] + auc_scores[\"Classifier\"],\n",
    "                       \"AUC\": [results[classifier_label][\"Training AUC\"]] + auc_scores[\"AUC\"],\n",
    "                       \"AUC Type\": [\"Training\"] + auc_scores[\"AUC Type\"]})\n",
    "    \n",
    "    auc_scores.update({\"Classifier\": [classifier_label] + auc_scores[\"Classifier\"],\n",
    "                       \"AUC\": [results[classifier_label][\"Test AUC\"]] + auc_scores[\"AUC\"],\n",
    "                       \"AUC Type\": [\"Test\"] + auc_scores[\"AUC Type\"]})\n",
    "\n",
    "# Dictionary to PandasDataFrame\n",
    "auc_scores = pd.DataFrame(auc_scores)\n",
    "\n",
    "# Set graph style\n",
    "sns.set(font_scale = 1.75)\n",
    "sns.set_style({\"axes.facecolor\": \"1.0\", \"axes.edgecolor\": \"0.85\", \"grid.color\": \"0.85\",\n",
    "               \"grid.linestyle\": \"-\", 'axes.labelcolor': '0.4', \"xtick.color\": \"0.4\",\n",
    "               'ytick.color': '0.4'})\n",
    "\n",
    "    \n",
    "# Colors\n",
    "training_color = sns.color_palette(\"RdYlBu\", 10)[1]\n",
    "test_color = sns.color_palette(\"RdYlBu\", 10)[-2]\n",
    "colors = [training_color, test_color]\n",
    "\n",
    "# Set figure size and create barplot\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "sns.barplot(x=\"AUC\", y=\"Classifier\", hue=\"AUC Type\", palette = colors,\n",
    "            data=auc_scores)\n",
    "\n",
    "# Generate a bolded horizontal line at y = 0\n",
    "ax.axvline(x = 0, color = 'black', linewidth = 4, alpha = .7)\n",
    "\n",
    "# Turn frame off\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "# Tight layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datafeed\n",
    "from nextrade.data import Node, Module, DataFeed, Stream, Select\n",
    "\n",
    "def preprocess(data: pd.DataFrame):\n",
    "    features = []    \n",
    "    features += [Stream(list(data.index)).rename(\"Timestamp\")]\n",
    "    for c in data.columns:\n",
    "        s = Stream(list(data[c])).rename(data[c].name)\n",
    "        features += [s]\n",
    "        \n",
    "    feed = DataFeed(features)\n",
    "    feed.compile()\n",
    "    return feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the best classifier in previous steps to be clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = results['QDA']['Classifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_proba = clf.predict_proba(X_train)\n",
    "y_test_proba = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_test_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('ml_results.pkl', 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('ml_results.pkl', 'rb') as handle:\n",
    "    results = pickle.load(handle)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test in Simulated Exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_time = pd.to_datetime('2020-01-01').date()\n",
    "train_end_time = pd.to_datetime('2020-02-15').date()\n",
    "test_start_time = pd.to_datetime('2020-07-15').date()\n",
    "test_end_time = pd.to_datetime('2020-08-15').date()\n",
    "\n",
    "train_data = data.loc[train_start_time:train_end_time]\n",
    "test_data = data.loc[test_start_time:test_end_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_end_time = pd.to_datetime('2020-08-16').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env_config = {'data':data, \n",
    "                    'start_time':train_start_time, \n",
    "                    'end_time':train_end_time, \n",
    "                    'preprocess':preprocess, \n",
    "                    'rl_env':False, \n",
    "                    'slice_len':-1,\n",
    "                    'use_internal':True,\n",
    "                    'exchange_name':'BITF',\n",
    "                    'renders':['plotly']}\n",
    "test_env_config = {'data':data, \n",
    "                    'start_time':test_start_time, \n",
    "                    'end_time':test_end_time, \n",
    "                    'preprocess':preprocess, \n",
    "                    'rl_env':False, \n",
    "                    'slice_len':-1,\n",
    "                    'use_internal':True,\n",
    "                    'exchange_name':'BITF',\n",
    "                    'renders':['plotly']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nextrade.environments import TradingEnvironment\n",
    "train_env = TradingEnvironment(train_env_config)\n",
    "test_env = TradingEnvironment(test_env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env.action_scheme.actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_pred(clf, obs, feature_set):\n",
    "    features = {x:obs[x] for x in feature_set}\n",
    "    feature_df = pd.DataFrame(features,[0])\n",
    "    pred = clf.predict(feature_df)[0]\n",
    "    pred_proba = clf.predict_proba(feature_df)[0]\n",
    "    return pred, pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Strategy():\n",
    "    def __init__(self, clf, feature_set):\n",
    "        self.clf = clf\n",
    "        self.feature_set = feature_set\n",
    "        \n",
    "        self.position = 0\n",
    "        self.last_price = -1\n",
    "        self.price_threshold = 0.02\n",
    "        \n",
    "    def check_if_run_model(self, obs):\n",
    "        self.position = obs['BITF:/BTC:/total']\n",
    "        \n",
    "        if_run = None\n",
    "        if self.position > 0 and \\\n",
    "           np.abs(obs['close']/self.last_price-1) > self.price_threshold:\n",
    "            if_run = False\n",
    "        else:\n",
    "            if_run = True\n",
    "            \n",
    "        return if_run\n",
    "        \n",
    "    def get_action(self, obs):\n",
    "        pred, pred_proba = get_model_pred(self.clf, obs, self.feature_set)\n",
    "        if pred_proba[1]>0.6:\n",
    "            action = 3\n",
    "            self.last_price = obs['close']\n",
    "        else:\n",
    "            action = 4\n",
    "            self.last_price = obs['close']\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run strategy on simulated exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "env = test_env\n",
    "\n",
    "start = time.time()\n",
    "strategy = Strategy(clf, selected_features)\n",
    "obs = env.reset()\n",
    "\n",
    "strat_time_list = []\n",
    "env_time_list = []\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "#for i in range(1000):\n",
    "    start = time.time()\n",
    "    if_run = strategy.check_if_run_model(obs)\n",
    "    if if_run:\n",
    "        action = strategy.get_action(obs)\n",
    "    else:\n",
    "        action = 0\n",
    "    end = time.time()\n",
    "    strat_time_list.append(end-start)\n",
    "    \n",
    "    start = time.time()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    end = time.time()\n",
    "    env_time_list.append(end-start)\n",
    "print(\"Strat time: {:.2f} s Env time: {:.2f} s\".format(np.sum(strat_time_list), np.sum(env_time_list)))\n",
    "print(\"Return rate: {:.2f}%\".format(100*obs['curr_return']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['net_worth','BITF:/USD-BTC','BITF:/BTC:/worth']\n",
    "env.portfolio.performance[metrics].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Test in Simulated Swap Exchange (NOT implemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_env_config = {'data':data, \n",
    "                    'start_time':train_start_time, \n",
    "                    'end_time':train_end_time, \n",
    "                    'preprocess':preprocess, \n",
    "                    'rl_env':False, \n",
    "                    'slice_len':-1,\n",
    "                    'use_internal':True,\n",
    "                    'exchange_name':'BITF',\n",
    "                    'renders':['plotly']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nextrade.environments import TradingEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import nextrade.rewards as rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nextrade.environments import SwapTradingEnvironment\n",
    "env = TradingEnvironment(train_env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "feed = preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.action_scheme.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action = 0\n",
    "    if obs['ma5m'] > obs['ma10m']:\n",
    "        action = 3\n",
    "    else:\n",
    "        action = 4\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    #print('Step {} action {}'.format(info['step'], action, obs['curr_return']))\n",
    "\n",
    "    end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "price_history = pd.concat([\n",
    "    cdd.fetch(\"Coinbase\", \"USD\", \"BTC\", \"1h\"),\n",
    "], axis=1)\n",
    "price_history = price_history.rename({\"date\": \"datetime\"}, axis=1)\n",
    "\n",
    "train_price_history = price_history[price_history['datetime'].between(train_start_time, train_end_time, inclusive=True)]\n",
    "train_price_history.reset_index(drop=True, inplace=True)\n",
    "test_price_history = price_history[price_history['datetime'].between(test_start_time, test_end_time, inclusive=True)]\n",
    "test_price_history.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Setup Trading Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training through Ray.Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "from wandb.ray import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.policy import Policy\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_start(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                         policies: Dict[str, Policy],\n",
    "                         episode: MultiAgentEpisode, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def on_episode_step(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                        episode: MultiAgentEpisode, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                       policies: Dict[str, Policy], episode: MultiAgentEpisode,\n",
    "                       **kwargs):\n",
    "        print(\"episode {} ended with return {}\".format(\n",
    "            episode.episode_id, episode.last_observation_for()[-1][-3]))\n",
    "        \n",
    "        episode.custom_metrics[\"return_rate\"] = episode.last_observation_for()[-1][-3]\n",
    "        episode.custom_metrics[\"yearly_return_rate\"] = episode.last_observation_for()[-1][-2]\n",
    "        \n",
    "\n",
    "    def on_sample_end(self, worker: RolloutWorker, samples: SampleBatch,\n",
    "                      **kwargs):\n",
    "        pass\n",
    "\n",
    "    def on_train_result(self, trainer, result: dict, **kwargs):\n",
    "        pass\n",
    "        #print(\"trainer.train() result: {} -> {} episodes\".format(\n",
    "        #    trainer, result[\"episodes_this_iter\"]))\n",
    "        # you can mutate the result dict to add new fields to return\n",
    "        result[\"callback_ok\"] = True\n",
    "\n",
    "    def on_postprocess_trajectory(\n",
    "            self, worker: RolloutWorker, episode: MultiAgentEpisode,\n",
    "            agent_id: str, policy_id: str, policies: Dict[str, Policy],\n",
    "            postprocessed_batch: SampleBatch,\n",
    "            original_batches: Dict[str, SampleBatch], **kwargs):\n",
    "        pass\n",
    "        #print(\"postprocessed {} steps\".format(postprocessed_batch.count))\n",
    "        if \"num_batches\" not in episode.custom_metrics:\n",
    "            episode.custom_metrics[\"num_batches\"] = 0\n",
    "        episode.custom_metrics[\"num_batches\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.evaluation.metrics import collect_episodes, summarize_episodes\n",
    "def eval_fn(trainer, eval_workers):\n",
    "    \"\"\"Example of a custom evaluation function.\n",
    "    Arguments:\n",
    "        trainer (Trainer): trainer class to evaluate.\n",
    "        eval_workers (WorkerSet): evaluation workers.\n",
    "    Returns:\n",
    "        metrics (dict): evaluation metrics dict.\n",
    "    \"\"\"\n",
    "    print(\"de\")\n",
    "    # We configured 2 eval workers in the training config.\n",
    "    worker_1, worker_2 = eval_workers.remote_workers()\n",
    "\n",
    "    # Set different env settings for each worker. Here we use a fixed config,\n",
    "    # which also could have been computed in each worker by looking at\n",
    "    # env_config.worker_index (printed in SimpleCorridor class above).\n",
    "    #worker_1.foreach_env.remote(lambda env: env.set_corridor_length(4))\n",
    "    #worker_2.foreach_env.remote(lambda env: env.set_corridor_length(7))\n",
    "\n",
    "    for i in range(5):\n",
    "        print(\"Custom evaluation round\", i)\n",
    "        # Calling .sample() runs exactly one episode per worker due to how the\n",
    "        # eval workers are configured.\n",
    "        ray.get([w.sample.remote() for w in eval_workers.remote_workers()])\n",
    "\n",
    "    # Collect the accumulated episodes on the workers, and then summarize the\n",
    "    # episode stats into a metrics dict.\n",
    "    episodes, _ = collect_episodes(\n",
    "        remote_workers=eval_workers.remote_workers(), timeout_seconds=99999)\n",
    "    # You can compute metrics from the episodes manually, or use the\n",
    "    # convenient `summarize_episodes()` utility:\n",
    "    metrics = summarize_episodes(episodes)\n",
    "    # Note that the above two statements are the equivalent of:\n",
    "    # metrics = collect_metrics(eval_workers.local_worker(),\n",
    "    #                           eval_workers.remote_workers())\n",
    "    print(metrics)\n",
    "    # You can also put custom values in the metrics dict.\n",
    "    metrics[\"foo\"] = 1\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.dqn import ApexTrainer\n",
    "from ray.rllib.agents import dqn\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from nextrade.environments import TradingEnvironment\n",
    "\n",
    "register_env(\"Trade\", lambda config: TradingEnvironment(config))\n",
    "\n",
    "config = dqn.apex.APEX_DEFAULT_CONFIG.copy()\n",
    "\n",
    "config[\"env\"] = \"Trade\"\n",
    "config[\"env_config\"] = train_env_config\n",
    "config[\"monitor\"] = True\n",
    "config[\"env_config\"][\"wandb\"] = {\"project\": \"test\", \"monitor_gym\": True}\n",
    "config[\"log_level\"] = \"ERROR\"\n",
    "config[\"timesteps_per_iteration\"] = 1440 * 4 * 1 \n",
    "config[\"target_network_update_freq\"] = 1000\n",
    "config[\"learning_starts\"] = 50000\n",
    "config[\"num_workers\"] = 4\n",
    "config[\"num_envs_per_worker\"] = 1\n",
    "config[\"callbacks\"] = MyCallbacks\n",
    "    \n",
    "config[\"evaluation_num_workers\"] = 2 \n",
    "config[\"custom_eval_function\"] = eval_fn\n",
    "config[\"evaluation_interval\"] = 1\n",
    "config[\"evaluation_num_episodes\"] = 1\n",
    "config[\"evaluation_config\"] = {\n",
    "            \"env\": \"Trade\",\n",
    "            \"env_config\": test_env_config\n",
    "        }\n",
    "\n",
    "ray.init(num_cpus=32)\n",
    "tune.run(ApexTrainer,\n",
    "            loggers=[],\n",
    "            config=config,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(env.feed.inputs[0]._array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(env.net_worth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    env.step(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.step(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(env.step(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward_scheme.get_reward(env._portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env._portfolio.performance['net_worth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward_scheme.window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wallet in env.portfolio._wallets:\n",
    "    print(env.portfolio._wallets[wallet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.feed.inputs[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset();\n",
    "for node in env.feed.inputs:\n",
    "    if isinstance(node, Stream):\n",
    "        print(len(node._array))\n",
    "        print(node._cursor)\n",
    "        node._cursor = 2\n",
    "        print(node._cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.portfolio._wallets[('ab499fcc-f5e9-49b6-9f61-7bc44ae64d34', 'USD')]._exchange.inputs[0]._cursor\n",
    "env.feed.inputs[0]._cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(env.feed._array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    env.feed.next()    \n",
    "    print(\"\")\n",
    "    print(\"obs: {}\".format(env.feed.value))\n",
    "    #print(\"ex: {}\".format(env.portfolio._wallets[('ab499fcc-f5e9-49b6-9f61-7bc44ae64d34', 'USD')]._exchange._prices['USD/BTC'].value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(1000):\n",
    "    env.step(0)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    env.feed.next()   \n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    env.feed.next()    \n",
    "    print(\"obs: {}\".format(env.feed.value['BITFINEX_BTC:close']))\n",
    "    print(\"ex: {}\".format(env.portfolio._wallets[('ab499fcc-f5e9-49b6-9f61-7bc44ae64d34', 'USD')]._exchange._prices['USD/BTC'].value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_scheme.action_space\n",
    "orders = env.action_scheme.get_order(1, env.portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.feed.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.exchanges._prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(env.action_scheme.exchange_pairs[0].price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(1000):\n",
    "    env.step(0)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "\n",
    "fig.show()\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_return_rate_list = []\n",
    "test_return_rate_list = []\n",
    "\n",
    "def test_strat(trainer, step, total_step):\n",
    "    train_env = TradingEnvironment(train_env_config)\n",
    "    obs = train_env.reset()\n",
    "    done = False\n",
    "\n",
    "    while done == False:\n",
    "        a = trainer.compute_action(obs)\n",
    "        obs, reward, done, info = train_env.step(action=a)\n",
    "\n",
    "    train_return_rate = train_env.portfolio.performance.net_worth.iloc[-1]/train_env.portfolio.performance.net_worth.iloc[0]\n",
    "    train_return_rate = pow(train_return_rate, 525600/len(train_data))-1\n",
    "    \n",
    "    test_env = TradingEnvironment(test_env_config)\n",
    "    obs = test_env.reset()\n",
    "    done = False\n",
    "\n",
    "    while done == False:\n",
    "        a = trainer.compute_action(obs)\n",
    "        obs, reward, done, info = test_env.step(action=a)\n",
    "        \n",
    "    test_return_rate = test_env.portfolio.performance.net_worth.iloc[-1]/test_env.portfolio.performance.net_worth.iloc[0]\n",
    "    test_return_rate = pow(test_return_rate, 525600/len(test_data))-1\n",
    "\n",
    "\n",
    "\n",
    "    train_return_rate_list.append(train_return_rate)\n",
    "    test_return_rate_list.append(test_return_rate)\n",
    "    \n",
    "    print(\"Episode {} has train return {} and test return {}\".format(step, train_return_rate, test_return_rate))\n",
    "    ax.clear()\n",
    "    ax.plot(train_return_rate_list,label='train')\n",
    "    ax.plot(test_return_rate_list,label='test')\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    #test_env.portfolio.performance.net_worth.plot()\n",
    "    #test_env.render(episode=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Data Feed Observation\n",
    "\n",
    "Even though this observation contains data from the internal data feed, since `use_internal=False` this data will not be provided as input to the observation history. The data that will be added to observation history of the environment will strictly be the nodes that have been included into the data feed that has been provided as a parameter to the trading environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Train DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nextrade.agents import DQNAgent\n",
    "#agent = DQNAgent(train_env_config = env_config,\\\n",
    "#                 test_env_config = env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.examples.env.repeat_after_me_env import RepeatAfterMeEnv\n",
    "from ray.rllib.examples.env.repeat_initial_obs_env import RepeatInitialObsEnv\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.trainer import with_common_config\n",
    "from ray.rllib.utils import merge_dicts\n",
    "from ray.rllib.utils.deprecation import deprecation_warning, DEPRECATED_VALUE\n",
    "from ray.rllib.agents.dqn.dqn import DQNTrainer, \\\n",
    "    DEFAULT_CONFIG as DQN_CONFIG, calculate_rr_weights\n",
    "from ray.rllib.agents.dqn.apex import ApexTrainer, APEX_DEFAULT_CONFIG\n",
    "\n",
    "\n",
    "APEX_DEFAULT_CONFIG = merge_dicts(\n",
    "    DQN_CONFIG,  # see also the options in dqn.py, which are also supported\n",
    "    {\n",
    "        \"optimizer\": merge_dicts(\n",
    "            DQN_CONFIG[\"optimizer\"], {\n",
    "                \"max_weight_sync_delay\": 400,\n",
    "                \"num_replay_buffer_shards\": 4,\n",
    "                \"debug\": False\n",
    "            }),\n",
    "        \"n_step\": 3,\n",
    "        \"num_gpus\": 1,\n",
    "        \"num_workers\": 32,\n",
    "        \"buffer_size\": 2000000,\n",
    "        \"learning_starts\": 50000,\n",
    "        \"train_batch_size\": 512,\n",
    "        \"rollout_fragment_length\": 50,\n",
    "        \"target_network_update_freq\": 500000,\n",
    "        \"timesteps_per_iteration\": 25000,\n",
    "        \"exploration_config\": {\"type\": \"PerWorkerEpsilonGreedy\"},\n",
    "        \"worker_side_prioritization\": True,\n",
    "        \"min_iter_time_s\": 30,\n",
    "        # If set, this will fix the ratio of replayed from a buffer and learned\n",
    "        # on timesteps to sampled from an environment and stored in the replay\n",
    "        # buffer timesteps. Otherwise, replay will proceed as fast as possible.\n",
    "        \"training_intensity\": None,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.agents.dqn as dqn\n",
    "\n",
    "data_length = len(data)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--num-cpus\", type=int, default=10)\n",
    "\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "try:\n",
    "    ray.shutdown()\n",
    "    ray.init(num_cpus=args.num_cpus+1 or None)\n",
    "except:\n",
    "    ray.init(num_cpus=args.num_cpus+1 or None)\n",
    "\n",
    "register_env(\"Trade\", lambda config: TradingEnvironment(train_env_config))\n",
    "\n",
    "config = APEX_DEFAULT_CONFIG\n",
    "\n",
    "config['num_workers'] = args.num_cpus\n",
    "#config['num_envs_per_worker'] = 5\n",
    "#config['sample_batch_size'] = data_length\n",
    "#config['train_batch_size'] = data_length\n",
    "#config['num_gpus'] = 1\n",
    "#config['timesteps_per_iteration'] = 0\n",
    "#config['framework'] = 'torch'\n",
    "\n",
    "trainer = dqn.ApexTrainer(env=\"Trade\", config=config)\n",
    "#results = tune.run(trainer, config=config, stop=stop, verbose=1)\n",
    "\n",
    "total_step = 100000\n",
    "\n",
    "for step in range(0,total_step):\n",
    "    results = trainer.train()\n",
    "    if step % 1 == 0:\n",
    "        test_strat(trainer, step, total_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
